#!/usr/bin/env python3
"""
RAGç®¡é“å®ç° - åŸºäºLangChainçš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ
åŠŸèƒ½ï¼šç”¨æˆ·æ¶ˆæ¯ â†’ æ£€ç´¢ç›¸å…³æ–‡æ¡£ â†’ ç”Ÿæˆå¸¦å¼•ç”¨çš„å›ç­”
"""
import os
import sys
import logging
import time
from typing import List, Dict, Any, Optional
from langchain_community.embeddings import DashScopeEmbeddings
from app.services.llm_config import create_llm_with_custom_config
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.messages import HumanMessage, AIMessage
from langchain_core.documents import Document
import json
from dotenv import load_dotenv
load_dotenv()

# ç¡®ä¿å½“å‰ç›®å½•åœ¨Pythonè·¯å¾„ä¸­
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# å¯¼å…¥ä¼šè¯ç®¡ç†å’Œæ—¥å¿—ç®¡ç†å™¨
try:
    from ..managers.session_manager import session_manager
    from ..managers.logger_manager import logger_manager
    from ..managers.redis_manager import redis_manager
    from ..managers.mysql_manager import mysql_manager
except ImportError as e:
    logging.warning(f"å¯¼å…¥ç®¡ç†å™¨æ¨¡å—å¤±è´¥: {e}")
    # åˆ›å»ºç©ºå¯¹è±¡é˜²æ­¢ç¨‹åºå´©æºƒ
    session_manager = None
    logger_manager = None
    redis_manager = None

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RAGPipeline:
    """RAGç®¡é“ç±» - æ£€ç´¢å¢å¼ºç”Ÿæˆ"""
    
    def __init__(self, collection_name="customer_policy"):
        """
        åˆå§‹åŒ–RAGç®¡é“
        
        Args:
            collection_name: Qdranté›†åˆåç§°
        """
        self.collection_name = collection_name
        self.vectorstore = None
        self.llm = None
        self.embeddings = None
        self._initialized = False
        self._init_error = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–RAGç®¡é“ç»„ä»¶"""
        try:
            # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - æ”¯æŒå¤šä¸ªç¯å¢ƒå˜é‡åç§°
            api_key = (os.getenv("DASHSCOPE_API_KEY") or 
                      os.getenv("BAILIAN_API_KEY") or 
                      os.getenv("OPENAI_API_KEY"))
            
            if api_key:
                self.embeddings = DashScopeEmbeddings(
                    model="text-embedding-v2",
                    dashscope_api_key=api_key
                )
                logger.info("ä½¿ç”¨DashScopeEmbeddingsåµŒå…¥æ¨¡å‹")
                logger.info(f"APIå¯†é’¥é•¿åº¦: {len(api_key)}")
            else:
                logger.warning("æœªæ‰¾åˆ°APIå¯†é’¥ï¼ˆDASHSCOPE_API_KEY/BAILIAN_API_KEYï¼‰ï¼Œä½¿ç”¨æ¨¡æ‹ŸåµŒå…¥")
                self.embeddings = MockEmbeddings()
            
            # 2. åŠ è½½å‘é‡æ•°æ®åº“ - ä½¿ç”¨Qdrant
            try:
                from app.services.knowledge_base import QdrantVectorStore
                
                self.vectorstore = QdrantVectorStore(
                    collection_name=self.collection_name
                )
                logger.info(f"æˆåŠŸåŠ è½½Qdrantå‘é‡æ•°æ®åº“: {self.collection_name}")
                
            except ImportError as e:
                logger.error(f"æ— æ³•å¯¼å…¥QdrantVectorStore: {e}")
                self.vectorstore = None
            except Exception as e:
                logger.error(f"åŠ è½½Qdrantå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
                self.vectorstore = None
            
            # 3. åˆå§‹åŒ–LLMï¼ˆä½¿ç”¨ç»Ÿä¸€çš„LLMé…ç½®ï¼‰
            try:
                # ä½¿ç”¨ç»Ÿä¸€çš„LLMé…ç½®
                self.llm = create_llm_with_custom_config(
                    temperature=0.1,
                    max_tokens=1000
                )
                
                if self.llm:
                    logger.info("åˆå§‹åŒ–çœŸå®LLMæ¨¡å‹")
                else:
                    logger.warning("æœªæ‰¾åˆ°APIå¯†é’¥æˆ–é…ç½®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                    self.llm = MockLLM()
            except Exception as e:
                logger.warning(f"åˆå§‹åŒ–LLMå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.llm = MockLLM()
            
            self._initialized = True
            logger.info(f"RAGç®¡é“åˆå§‹åŒ–å®Œæˆï¼ŒCollection: {self.collection_name}")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–RAGç®¡é“å¤±è´¥: {e}")
            self._init_error = str(e)
            self._initialized = False
    
    def is_available(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥RAGç®¡é“æ˜¯å¦å¯ç”¨
        
        Returns:
            å¯ç”¨æ€§çŠ¶æ€
        """
        return {
            "initialized": self._initialized,
            "collection_name": self.collection_name,
            "vectorstore_available": self.vectorstore is not None,
            "embeddings_available": self.embeddings is not None,
            "llm_available": self.llm is not None,
            "error": self._init_error
        }
    
    def retrieve_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹æ£€ç´¢æ–‡æ¡£ï¼ŒæŸ¥è¯¢: {query}, top_k: {top_k}")
            logger.info(f"RAGç®¡é“çŠ¶æ€: {self.is_available()}")
            
            if not self.vectorstore:
                logger.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ£€ç´¢æ–‡æ¡£")
                return []
            
            docs = self.vectorstore.search_knowledge(query, limit=top_k)
            logger.info(f"çŸ¥è¯†åº“æœç´¢è¿”å› {len(docs)} ä¸ªæ–‡æ¡£")
            
            results = []
            for i, doc in enumerate(docs):
                result = {
                    "content": doc['content'],
                    "metadata": {
                        "source": doc['source'],
                        "section": doc.get('section', '')
                    },
                    "score": doc['score']
                }
                results.append(result)
                logger.info(f"æ–‡æ¡£ {i+1}: ç›¸ä¼¼åº¦åˆ†æ•°={result['score']}, æºæ–‡ä»¶={result['metadata'].get('source', 'æœªçŸ¥')}")
                logger.debug(f"æ–‡æ¡£ {i+1} å†…å®¹é¢„è§ˆ: {result['content'][:100]}...")
            
            logger.info(f"æˆåŠŸæ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            return results
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
            return []
    
    def generate_response(self, query: str, retrieved_docs: List[Dict]) -> str:
        """
        åŸºäºæ£€ç´¢çš„æ–‡æ¡£ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        if not retrieved_docs:
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚\n\nğŸ’¡ å»ºè®®ï¼š\nâ€¢ è¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜\nâ€¢ å¦‚æœæ¶‰åŠå…·ä½“è®¢å•æˆ–ç‰©æµï¼Œè¯·æä¾›è®¢å•å·æˆ–å¿«é€’å•å·\nâ€¢ æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥è”ç³»å®¢æœçƒ­çº¿ï¼š400-123-4567 è·å–äººå·¥å¸®åŠ©"
        
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context(retrieved_docs)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(query, context)
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            if isinstance(self.llm, MockLLM):
                # æ¨¡æ‹ŸLLMå›ç­”
                response = self.llm.generate(prompt, retrieved_docs)
            else:
                # çœŸå®LLMè°ƒç”¨
                messages = [HumanMessage(content=prompt)]
                response = self.llm.invoke(messages)
                response = response.content
            
            return response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {e}"
    
    def _build_context(self, docs: List[Dict]) -> str:
        """æ„å»ºæ£€ç´¢æ–‡æ¡£çš„ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, doc in enumerate(docs, 1):
            source = doc.get("metadata", {}).get("source", f"æ–‡æ¡£{i}")
            content = doc["content"]
            context_parts.append(f"=== {source} ===\n{content}")
        
        return "\n\n".join(context_parts)
    
    async def generate_response_with_context(self, query: str, retrieved_docs: List[Dict], 
                                           context_prompt: str = "", 
                                           conversation_context: List[Dict] = None) -> str:
        """
        åŸºäºæ£€ç´¢çš„æ–‡æ¡£å’Œä¼šè¯ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£
            context_prompt: ä¼šè¯ä¸Šä¸‹æ–‡æç¤ºè¯
            conversation_context: å¯¹è¯å†å²ä¸Šä¸‹æ–‡
            
        Returns:
            ç”Ÿæˆçš„å›ç­”
        """
        if not retrieved_docs:
            return "æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚\n\nğŸ’¡ å»ºè®®ï¼š\nâ€¢ è¯·å°è¯•é‡æ–°æè¿°æ‚¨çš„é—®é¢˜\nâ€¢ å¦‚æœæ¶‰åŠå…·ä½“è®¢å•æˆ–ç‰©æµï¼Œè¯·æä¾›è®¢å•å·æˆ–å¿«é€’å•å·\nâ€¢ æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥è”ç³»å®¢æœçƒ­çº¿ï¼š400-123-4567 è·å–äººå·¥å¸®åŠ©"
        
        try:
            # æ„å»ºæ–‡æ¡£ä¸Šä¸‹æ–‡
            context = self._build_context(retrieved_docs)
            
            # æ„å»ºæç¤ºè¯
            if conversation_context and context_prompt:
                # æœ‰ä¼šè¯ä¸Šä¸‹æ–‡çš„æƒ…å†µ
                prompt = f"""{context_prompt}

                åŸºäºä»¥ä¸‹ç›¸å…³æ”¿ç­–æ–‡æ¡£å›ç­”ï¼š
                {context}

                è¯·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼Œå¹¶å¼•ç”¨å…·ä½“çš„æ”¿ç­–æ¡æ¬¾ã€‚"""
            else:
                # æ²¡æœ‰ä¼šè¯ä¸Šä¸‹æ–‡çš„æƒ…å†µ
                prompt = self._build_prompt(query, context)
            
            # è°ƒç”¨LLMç”Ÿæˆå›ç­”
            if isinstance(self.llm, MockLLM):
                # æ¨¡æ‹ŸLLMå›ç­”
                response = self.llm.generate(prompt, retrieved_docs)
            else:
                # çœŸå®LLMè°ƒç”¨
                messages = [HumanMessage(content=prompt)]
                response = self.llm.invoke(messages)
                response = response.content
            
            return response
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆä¸Šä¸‹æ–‡å›ç­”å¤±è´¥: {e}")
            return f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {e}"
    
    def _build_prompt(self, query: str, context: str) -> str:
        """æ„å»ºæç¤ºè¯"""
        prompt = f"""ä½ æ˜¯å®¢æœåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ”¿ç­–æ–‡æ¡£å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

ç›¸å…³æ”¿ç­–æ–‡æ¡£ï¼š
{context}

è¯·éµå¾ªä»¥ä¸‹ä¸¥æ ¼è¦æ±‚ï¼š
1. åªèƒ½åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œå¿…é¡»æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·"æœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³å†…å®¹"
3. ä¸¥æ ¼ç¦æ­¢ç¼–é€ ã€æ¨æµ‹æˆ–è™šæ„ä»»ä½•ä¸åœ¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯
4. å›ç­”è¦ä¸“ä¸šã€å‹å¥½
5. å¼•ç”¨å…·ä½“çš„æ”¿ç­–æ¡æ¬¾ï¼ˆå¦‚æœæœ‰ï¼‰
6. å¦‚æœæ¶‰åŠå…·ä½“æ“ä½œæŒ‡å¼•ï¼Œè¯·è¯¦ç»†è¯´æ˜ï¼ˆåŸºäºæ–‡æ¡£å†…å®¹ï¼‰

å›ç­”ï¼š"""
        return prompt
    
    async def process_message(self, message: str, session_id: str = None, 
                            conversation_context: List[Dict] = None) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯çš„æµç¨‹ - ä»…ä½œä¸ºä¿¡æ¯æ£€ç´¢å·¥å…·
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            session_id: ä¼šè¯ID
            conversation_context: å¯¹è¯å†å²ä¸Šä¸‹æ–‡
            
        Returns:
            åŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œå¼•ç”¨ä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        logger.info(f"å¤„ç†ç”¨æˆ·æ¶ˆæ¯: {message}")
        
        try:
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.retrieve_documents(message, top_k=5)
            
            # åœ¨æ§åˆ¶å°è¾“å‡ºæ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œæ–¹ä¾¿è°ƒè¯•
            print("\n=== å‘é‡çŸ¥è¯†åº“æ£€ç´¢ç»“æœ ===")
            print(f"æŸ¥è¯¢: {message}")
            print(f"æ£€ç´¢åˆ°æ–‡æ¡£æ•°é‡: {len(retrieved_docs)}")
            
            for i, doc in enumerate(retrieved_docs, 1):
                print(f"\næ–‡æ¡£ {i}:")
                print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {doc.get('score', 'æœªçŸ¥')}")
                print(f"æºæ–‡ä»¶: {doc.get('metadata', {}).get('source', 'æœªçŸ¥')}")
                print(f"å†…å®¹: {doc['content'][:200]}{'...' if len(doc['content']) > 200 else ''}")
            
            print("==========================\n")
            
            # 2. æ„å»ºå¼•ç”¨ä¿¡æ¯
            references = []
            for doc in retrieved_docs:
                source = doc.get("metadata", {}).get("source", "æœªçŸ¥æ¥æº")
                references.append({
                    "source": source,
                    "content_preview": doc["content"][:100] + "..." if len(doc["content"]) > 100 else doc["content"],
                    "score": doc.get("score", None)
                })
            
            # 3. è¿”å›ç»“æœ
            result = {
                "documents": retrieved_docs,  # è¿”å›åŸå§‹æ£€ç´¢æ–‡æ¡£
                "references": references,     # ç®€åŒ–çš„å¼•ç”¨ä¿¡æ¯
                "query": message,
                "session_id": session_id,
                "has_knowledge": len(retrieved_docs) > 0,
                "processing_time": time.time() - start_time,
                "retrieved_count": len(retrieved_docs)
            }
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")
            return {
                "documents": [],
                "references": [],
                "query": message,
                "session_id": session_id,
                "has_knowledge": False,
                "error": str(e)
            }
    
    def get_knowledge_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            from app.services.knowledge_base import QdrantVectorStore
            
            qdrant_store = QdrantVectorStore(collection_name=self.collection_name)
            collections = qdrant_store.client.get_collections()
            collection_info = next((c for c in collections.collections if c.name == self.collection_name), None)
            
            count = collection_info.points_count if collection_info else 0
            
            embedding_info = "DashScopeEmbeddings" if isinstance(self.embeddings, DashScopeEmbeddings) else "MockEmbeddings"
            
            return {
                "total_documents": count,
                "collection_name": self.collection_name,
                "vectorstore_type": "Qdrant",
                "host": qdrant_store.host,
                "port": qdrant_store.port,
                "embedding_model": embedding_info,
                "is_vectorstore_loaded": self.vectorstore is not None
            }
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "has_knowledge": False,
                "error": str(e)
            }
    
    async def aget_relevant_documents(self, query: str, top_k: int = 3):
        """
        å¼‚æ­¥è·å–ç›¸å…³æ–‡æ¡£ï¼ˆä¸LangChainæ¥å£å…¼å®¹ï¼‰
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            top_k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡ŒçŸ¥è¯†åº“æœç´¢
            results = self.vectorstore.search_knowledge(query, limit=top_k)
            logger.info(f"å¼‚æ­¥æ£€ç´¢åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # å°†æœç´¢ç»“æœè½¬æ¢ä¸ºDocumentå¯¹è±¡ä»¥ä¿æŒä¸LangChainæ¥å£å…¼å®¹
            from langchain_core.documents import Document
            docs = []
            for result in results:
                doc = Document(
                    page_content=result['content'],
                    metadata={
                        'source': result['source'],
                        'section': result.get('section', ''),
                        'score': result['score']
                    }
                )
                docs.append(doc)
            
            return docs
        except Exception as e:
            logger.error(f"å¼‚æ­¥æ£€ç´¢æ–‡æ¡£å¤±è´¥: {e}")
            return []


class MockLLM:
    """æ¨¡æ‹ŸLLMç±» - ç”¨äºæ¼”ç¤º"""
    
    def generate(self, prompt: str, retrieved_docs: List[Dict]) -> str:
        """æ¨¡æ‹Ÿç”Ÿæˆå›ç­”"""
        # ç®€å•çš„è§„åˆ™ç”Ÿæˆå›ç­”
        if "é€€è´§" in prompt or "é€€æ¢è´§" in prompt:
            if retrieved_docs:
                source = retrieved_docs[0].get("metadata", {}).get("source", "æ”¿ç­–æ–‡æ¡£")
                return f"æ ¹æ®{source}ï¼Œå•†å“æ”¶åˆ°å7å¤©å†…å¯ç”³è¯·é€€æ¢è´§ã€‚å•†å“å¿…é¡»ä¿æŒåŸåŒ…è£…å®Œæ•´ï¼Œæœªç»ä½¿ç”¨ã€‚å…·ä½“æµç¨‹ï¼š\n1. è”ç³»å®¢æœæå‡ºé€€æ¢è´§ç”³è¯·\n2. æä¾›è®¢å•å·å’Œé€€æ¢è´§åŸå› \n3. å®¢æœå®¡æ ¸é€šè¿‡åæä¾›é€€è´§åœ°å€\n4. å®¢æˆ·å¯„å›å•†å“å¹¶æä¾›å¿«é€’å•å·\n5. ä»“åº“æ”¶åˆ°å•†å“å3ä¸ªå·¥ä½œæ—¥å†…å¤„ç†"
            else:
                return "å…³äºé€€æ¢è´§æ”¿ç­–ï¼Œå»ºè®®æ‚¨è”ç³»æˆ‘ä»¬çš„å®¢æœè·å¾—è¯¦ç»†ä¿¡æ¯ã€‚"
        
        elif "å®¢æœæ—¶é—´" in prompt or "å·¥ä½œæ—¶é—´" in prompt:
            return "æˆ‘ä»¬çš„å®¢æœæ—¶é—´ä¸ºï¼šå·¥ä½œæ—¥9:00-18:00ã€‚ç´§æ€¥æƒ…å†µæœ‰24å°æ—¶å“åº”æœºåˆ¶ï¼ŒèŠ‚å‡æ—¥æœŸé—´ä¹Ÿæä¾›å€¼ç­æœåŠ¡ã€‚"
        
        elif "é€€æ¬¾" in prompt:
            return "é€€æ¬¾è¯´æ˜ï¼šé€€æ¬¾å°†åœ¨æ”¶åˆ°å•†å“å5-7ä¸ªå·¥ä½œæ—¥å†…å¤„ç†ï¼ŒåŸè·¯è¿”å›æ”¯ä»˜æ–¹å¼ã€‚è¿è´¹ä¸€èˆ¬ä¸äºˆé€€è¿˜ï¼ˆè´¨é‡é—®é¢˜é™¤å¤–ï¼‰ã€‚"
        
        else:
            return "æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ã€‚å»ºè®®æ‚¨æä¾›æ›´å…·ä½“çš„é—®é¢˜ï¼Œæˆ‘å°†æ ¹æ®æ”¿ç­–æ–‡æ¡£ä¸ºæ‚¨æä¾›å‡†ç¡®çš„ä¿¡æ¯ã€‚"
    
    def invoke(self, messages):
        """æ¨¡æ‹ŸLLMè°ƒç”¨"""
        class MockResponse:
            def __init__(self, content):
                self.content = content
        
        prompt = messages[0].content
        response = self.generate(prompt, [])
        return MockResponse(response)


class MockEmbeddings:
    """æ¨¡æ‹ŸåµŒå…¥ç±» - ç”Ÿæˆ1536ç»´å‘é‡ä»¥åŒ¹é…text-embedding-v2"""
    
    def embed_query(self, text):
        import hashlib
        import math
        
        # ç”Ÿæˆ1536ç»´çš„å‘é‡
        hash_val = hashlib.md5(text.encode()).digest()
        vector = []
        
        # ä½¿ç”¨å“ˆå¸Œå€¼ç”Ÿæˆ1536ç»´çš„å‘é‡
        for i in range(1536):
            byte_val = hash_val[i % len(hash_val)]
            # ç”Ÿæˆ-1åˆ°1ä¹‹é—´çš„å€¼
            value = (byte_val / 255.0) * 2 - 1
            vector.append(value)
        
        return vector
    
    def embed_documents(self, texts):
        return [self.embed_query(text) for text in texts]


def main():
    """ä¸»å‡½æ•° - æµ‹è¯•RAGç®¡é“"""
    # åˆ›å»ºRAGç®¡é“å®ä¾‹
    rag = RAGPipeline()
    
    # è·å–çŸ¥è¯†åº“ç»Ÿè®¡
    stats = rag.get_knowledge_stats()
    print(f"çŸ¥è¯†åº“ç»Ÿè®¡: {stats}")
    
    # æµ‹è¯•æ¶ˆæ¯
    test_messages = [
        "æˆ‘æƒ³è¦é€€è´§ï¼Œéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ",
        "å®¢æœä»€ä¹ˆæ—¶å€™ä¸Šç­ï¼Ÿ",
        "é€€æ¬¾éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ",
        "æˆ‘å¯ä»¥æ¢è´§å—ï¼Ÿ"
    ]
    
    print("\n=== RAGç®¡é“æµ‹è¯• ===")
    for message in test_messages:
        print(f"\nç”¨æˆ·: {message}")
        result = rag.process_message(message)
        print(f"AI: {result['response']}")
        print(f"å¼•ç”¨: {len(result['references'])} ä¸ªæ–‡æ¡£")
        for ref in result['references']:
            print(f"  - {ref['source']}")


if __name__ == "__main__":
    main()